{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "def load_data(fpath):\n",
    "    df = pd.read_csv(fpath, sep='\\t', index_col='participant_id')\n",
    "    df = df.dropna(subset=[TARGET_VAR], axis='index')\n",
    "    if PROBLEM_TYPE == 'classification':\n",
    "        df[TARGET_VAR] = df[TARGET_VAR].astype(bool)\n",
    "    elif PROBLEM_TYPE == 'regression':\n",
    "        df = df.query('AGE >= 55')\n",
    "        # df = df[['AGE', 'SEX', 'Left-Lateral-Ventricle', 'Right-Lateral-Ventricle', 'Right-Hippocampus', 'Left-Hippocampus']]\n",
    "    if 'SEX' in df.columns:\n",
    "        df.loc[:, 'SEX'] = df['SEX'].astype('category')\n",
    "    return df\n",
    "\n",
    "PROBLEM_TYPE = 'classification'\n",
    "# PROBLEM_TYPE = 'regression'\n",
    "N_FOLDS = 5\n",
    "RNG_SEED = 3791\n",
    "\n",
    "DATASET_COLOUR_MAP = {\n",
    "    'PPMI': '#D0A441',\n",
    "    'ADNI': '#0CA789',\n",
    "    'QPN': '#A6A6C6',\n",
    "}\n",
    "\n",
    "if PROBLEM_TYPE == 'classification':\n",
    "    TARGET_VAR = 'COG_DECLINE'\n",
    "    COMMON_TAGS = 'decline-age-case-aparc'\n",
    "elif PROBLEM_TYPE == 'regression':\n",
    "    TARGET_VAR = 'AGE'\n",
    "    COMMON_TAGS = 'age-sex-hc-aseg'\n",
    "else:\n",
    "    raise ValueError('PROBLEM_TYPE must be either classification or regression')\n",
    "\n",
    "ENV_VARS = dotenv_values('.env')\n",
    "DPATH_DATA = Path(ENV_VARS['DPATH_FL_DATA'])\n",
    "DPATH_FIGS = Path(ENV_VARS['DPATH_FL_FIGS'])\n",
    "DPATH_RESULTS = Path(ENV_VARS['DPATH_FL_RESULTS'])\n",
    "\n",
    "df_ppmi = load_data(DPATH_DATA / f'ppmi-{COMMON_TAGS}.tsv')\n",
    "df_adni = load_data(DPATH_DATA / f'adni-{COMMON_TAGS}.tsv')\n",
    "df_qpn = load_data(DPATH_DATA / f'qpn-{COMMON_TAGS}.tsv')\n",
    "df_all = pd.concat(\n",
    "    {\n",
    "        'ppmi': df_ppmi, \n",
    "        'adni': df_adni, \n",
    "        'qpn': df_qpn,\n",
    "    },\n",
    "    axis=0,\n",
    ")\n",
    "df_all.index.names = ['dataset', df_all.index.names[-1]]\n",
    "\n",
    "for label, df in zip(['ppmi', 'adni', 'qpn', 'all'], [df_ppmi, df_adni, df_qpn, df_all]):\n",
    "# for label, df in zip(['ppmi', 'qpn', 'all'], [df_ppmi, df_qpn, df_all]):\n",
    "    print(f'{label}: {df.shape}')\n",
    "    if PROBLEM_TYPE == 'classification':\n",
    "        print(f'\\t{TARGET_VAR}: {df[TARGET_VAR].value_counts(dropna=False).to_dict()}')\n",
    "\n",
    "# reorder columns (needed for coefficient comparison/averaging)\n",
    "df_ppmi = df_ppmi[df_all.columns]\n",
    "df_adni = df_adni[df_all.columns]\n",
    "df_qpn = df_qpn[df_all.columns]\n",
    "\n",
    "DF_RESULTS_TO_SAVE = pd.DataFrame(columns=['method', 'is_null', 'test_dataset', 'metric', 'i_fold', 'score'])\n",
    "\n",
    "def add_scores(method, test_dataset, metric, scores, is_null):\n",
    "    for i_fold, score in enumerate(scores):\n",
    "        DF_RESULTS_TO_SAVE.loc[len(DF_RESULTS_TO_SAVE)] = [method, is_null, test_dataset, metric, i_fold, score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='ticks')\n",
    "\n",
    "if PROBLEM_TYPE == 'regression':\n",
    "    fig, ax = plt.subplots(figsize=(2, 6))\n",
    "\n",
    "    df_to_plot = df_all.reset_index().copy()\n",
    "    df_to_plot['dataset'] = df_to_plot['dataset'].map(lambda x: x.upper())\n",
    "    sns.kdeplot(\n",
    "        data=df_to_plot,\n",
    "        y=TARGET_VAR,\n",
    "        hue='dataset',\n",
    "        ax=ax,\n",
    "        palette=DATASET_COLOUR_MAP,\n",
    "        fill=True,\n",
    "        linewidth=0,\n",
    "        alpha=0.5,\n",
    "        common_norm=False,\n",
    "    )\n",
    "    ax.set_ylabel('Age')\n",
    "    legend = ax.get_legend()\n",
    "    legend.set_title('')\n",
    "    legend.set_loc('lower right')\n",
    "    legend.get_frame().set_linewidth(0.0)\n",
    "    ax.axhline(y=55, color='grey', linestyle='--')\n",
    "    sns.despine(ax=ax)\n",
    "    xticks = [0, 0.05]\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels([f'{x:.2f}' for x in xticks])\n",
    "    ax.set_ylim(bottom=15, top=95)\n",
    "\n",
    "elif PROBLEM_TYPE == 'classification':\n",
    "    df_to_plot = df_all.reset_index().copy()\n",
    "    df_to_plot['dataset'] = df_to_plot['dataset'].map(lambda x: x.upper())\n",
    "    df_to_plot[TARGET_VAR] = df_to_plot[TARGET_VAR].map({True: 'Cog. decline', False: 'Stable'})\n",
    "    fig = sns.catplot(\n",
    "        data=df_to_plot,\n",
    "        y=TARGET_VAR,\n",
    "        row='dataset',\n",
    "        kind='count',\n",
    "        sharex=False,\n",
    "        height=2,\n",
    "        aspect=1.5,\n",
    "        palette=DATASET_COLOUR_MAP,\n",
    "        hue='dataset',\n",
    "        legend=False,\n",
    "        saturation=1,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    xticks = {'PPMI': [0, 1000], 'ADNI': [0, 400], 'QPN': [0, 40]}\n",
    "    for i_ax, (dataset, ax) in enumerate(fig.axes_dict.items()):\n",
    "        print(f'{dataset}: {ax.get_xlim()[1]}')\n",
    "        ax.set_ylabel('')\n",
    "        # ax.set_yticklabels(['Stable', 'Decline'])\n",
    "        # ax.set_xlabel('Count')\n",
    "        ax.set_title(dataset)\n",
    "        ax.set_xticks(xticks[dataset])\n",
    "        ax.set_xticklabels(xticks[dataset])\n",
    "        if i_ax == len(fig.axes_dict) - 1:\n",
    "            ax.set_xlabel('Count')\n",
    "        ax.containers[i_ax].patches[1].set_alpha(0.4)\n",
    "\n",
    "else:\n",
    "    raise ValueError('PROBLEM_TYPE must be either classification or regression')\n",
    "\n",
    "DPATH_FIGS.mkdir(exist_ok=True)\n",
    "\n",
    "fpath_out = DPATH_FIGS / f'data-{PROBLEM_TYPE}.png'\n",
    "# fig.savefig(fpath_out, dpi=300, bbox_inches='tight')\n",
    "print(fpath_out)\n",
    "\n",
    "# for dataset in ['ppmi', 'adni', 'qpn']:\n",
    "#     df_all.loc[dataset, 'AGE'].hist(ax=ax, alpha=0.5, label=dataset)\n",
    "# ax.legend()\n",
    "# fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableReport\n",
    "TableReport(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base ML model and helper function for fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, GradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV, SGDClassifier, SGDRegressor, LassoCV, RidgeCV, LinearRegression\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, GridSearchCV, KFold, GroupKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, r2_score, root_mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "from skrub import tabular_learner\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def neg_root_mean_squared_error(*args, **kwargs):\n",
    "    return -root_mean_squared_error(*args, **kwargs)\n",
    "\n",
    "def neg_mean_absolute_error(*args, **kwargs):\n",
    "    return -mean_absolute_error(*args, **kwargs)\n",
    "\n",
    "def get_metrics_map(include_roc_auc=True):\n",
    "    if PROBLEM_TYPE == 'classification':\n",
    "        metrics_dict = {\n",
    "            \"balanced_accuracy\": balanced_accuracy_score,\n",
    "        }\n",
    "        # if include_roc_auc:\n",
    "        #     # invalid, this is getting the preds instead of the scores\n",
    "        #     metrics_dict[\"roc_auc\"] = roc_auc_score\n",
    "    elif PROBLEM_TYPE == 'regression':\n",
    "        metrics_dict = {\n",
    "            \"r2\": r2_score,\n",
    "            \"neg_mean_absolute_error\": neg_mean_absolute_error,\n",
    "            \"explained_variance\": explained_variance_score,\n",
    "            'corr': (lambda x, y: pearsonr(np.squeeze(x), np.squeeze(y))[0]),\n",
    "            \"mean_absolute_error\": mean_absolute_error,\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"PROBLEM_TYPE must be either 'classification' or 'regression'\")\n",
    "    return metrics_dict\n",
    "\n",
    "def get_X_y(df_Xy):\n",
    "    y = df_Xy[TARGET_VAR].dropna()\n",
    "    X = df_Xy.drop(columns=TARGET_VAR).loc[y.index]\n",
    "    return X, y\n",
    "\n",
    "def ml_helper(df_Xy, pipeline, null=False, cv=None):\n",
    "\n",
    "    metrics = get_metrics_map()\n",
    "    for metric_name in metrics:\n",
    "        metrics[metric_name] = make_scorer(metrics[metric_name])\n",
    "\n",
    "    X, y = get_X_y(df_Xy)\n",
    "\n",
    "    if null:\n",
    "        y = y.sample(frac=1, replace=False, random_state=RNG_SEED)\n",
    "    \n",
    "    if cv is None:\n",
    "        if PROBLEM_TYPE == 'classification':\n",
    "            cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RNG_SEED)\n",
    "        elif PROBLEM_TYPE == 'regression':\n",
    "            cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RNG_SEED)\n",
    "            cv = cv.split(X, get_age_groups(df_Xy))\n",
    "        else:\n",
    "            raise ValueError(f\"PROBLEM_TYPE must be either 'classification' or 'regression'\")\n",
    "\n",
    "    results = cross_validate(\n",
    "        pipeline,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "        return_estimator=True,\n",
    "        return_indices=True,\n",
    "        scoring=metrics,\n",
    "    )\n",
    "    results['X'] = X\n",
    "    results['y'] = y\n",
    "    if metrics is None:\n",
    "        metrics = ['score']\n",
    "    for metric in metrics:\n",
    "        train_scores = results[f\"train_{metric}\"]\n",
    "        test_scores = results[f\"test_{metric}\"]\n",
    "        print(f\"{metric}\")\n",
    "        print(\n",
    "            f\"\\ttrain: {train_scores.mean():.2f} ({train_scores.std():.2f})\\t {train_scores}\"\n",
    "        )\n",
    "        print(f\"\\ttest: {test_scores.mean():.2f} ({test_scores.std():.2f})\\t {test_scores}\")\n",
    "    return results\n",
    "\n",
    "def get_fake_X_y():\n",
    "    X_fake = df_all.drop(columns=TARGET_VAR).iloc[[0] * N_FOLDS * 2]\n",
    "    y_fake = pd.Series([0, 1] * N_FOLDS)\n",
    "    if 'SEX' in X_fake.columns:\n",
    "        index_cols = X_fake.index.names\n",
    "        X_fake = X_fake.reset_index()\n",
    "        X_fake.loc[X_fake.index[0], 'SEX'] = 0\n",
    "        X_fake.loc[X_fake.index[1], 'SEX'] = 1\n",
    "        X_fake = X_fake.set_index(index_cols)\n",
    "    return X_fake, y_fake\n",
    "\n",
    "def get_age_groups(df, bin_size=10):\n",
    "    min_age = df['AGE'].min() // bin_size * bin_size\n",
    "    max_age = (df['AGE'].max() // bin_size + 1) * bin_size\n",
    "    bins = np.arange(min_age, max_age+1, bin_size)\n",
    "    labels = [f\"{int(bins[i])}-{int(bins[i+1])}\" for i in range(len(bins)-1)]\n",
    "    age_groups = pd.cut(df['AGE'], bins=bins, labels=labels)\n",
    "    # age_groups = pd.cut(df['AGE'], bins=2)# labels=labels)\n",
    "    return age_groups\n",
    "\n",
    "if PROBLEM_TYPE == 'classification':\n",
    "    model = LogisticRegressionCV(max_iter=1000, cv=StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RNG_SEED), class_weight=\"balanced\")\n",
    "    # model = LogisticRegressionCV(max_iter=1000, cv=StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RNG_SEED), penalty='l1', solver='saga', class_weight=\"balanced\")\n",
    "    pipeline = tabular_learner(model)\n",
    "    pipeline.set_params(simpleimputer__add_indicator=False, standardscaler='passthrough')\n",
    "elif PROBLEM_TYPE == 'regression':\n",
    "    # model = HistGradientBoostingRegressor(max_iter=5)\n",
    "    # model = LinearRegression()\n",
    "    model = RidgeCV(alphas=np.linspace(0.1, 10000, 100))#(cv=KFold(n_splits=N_FOLDS, shuffle=True, random_state=RNG_SEED))#, alphas=(0.1, 1, 10, 100, 1000))\n",
    "    # model = LassoCV(cv=KFold(n_splits=N_FOLDS, shuffle=True, random_state=RNG_SEED), max_iter=10000)\n",
    "    # model = PLSRegression(n_components=2)\n",
    "    pipeline = tabular_learner(model)\n",
    "    if 'simpleimputer' in pipeline.named_steps:\n",
    "        pipeline.set_params(simpleimputer__add_indicator=False)\n",
    "    # if 'standardscaler' in pipeline.named_steps:\n",
    "    #     pipeline.set_params(standardscaler='passthrough')\n",
    "else:\n",
    "    raise ValueError(f\"PROBLEM_TYPE must be either 'classification' or 'regression'\")\n",
    "\n",
    "pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: individual datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "print(f\"===== PPMI (sample size {len(df_ppmi)}) =====\")\n",
    "results_ppmi = ml_helper(df_ppmi, pipeline)\n",
    "\n",
    "print(f\"===== ADNI (sample size {len(df_adni)}) =====\")\n",
    "results_adni = ml_helper(df_adni, pipeline)\n",
    "\n",
    "print(f\"===== QPN (sample size {len(df_qpn)}) =====\")\n",
    "results_qpn = ml_helper(df_qpn, pipeline)\n",
    "\n",
    "results_all = defaultdict(list)\n",
    "for metric_name, metric_func in get_metrics_map().items():\n",
    "    for i_fold in range(N_FOLDS):\n",
    "        y_test_all = []\n",
    "        y_pred_all = []\n",
    "        for results, test_dataset in zip([results_ppmi, results_adni, results_qpn], ['ppmi', 'adni', 'qpn']):\n",
    "        # for results, test_dataset in zip([results_ppmi, results_qpn], ['ppmi', 'qpn']):\n",
    "            estimator_silo = results['estimator'][i_fold]\n",
    "            idx_test = results['indices']['test'][i_fold]\n",
    "            X_test = results['X'].iloc[idx_test]\n",
    "            y_test = results['y'].iloc[idx_test]\n",
    "            if metric_name == 'roc_auc':\n",
    "                # technically scores but renaming to pred for concatenation\n",
    "                y_test_pred = pd.Series(estimator_silo.predict_proba(X_test)[:, 1])\n",
    "                y_test_pred.index = y_test_pred.index\n",
    "            else:\n",
    "                y_test_pred = pd.Series(estimator_silo.predict(X_test))\n",
    "                y_test_pred.index = y_test.index\n",
    "            y_test_all.append(y_test)\n",
    "            y_pred_all.append(y_test_pred)\n",
    "        y_test_all_concat = pd.concat(y_test_all)\n",
    "        y_pred_all_concat = pd.concat(y_pred_all)\n",
    "        results_all[f'test_{metric_name}'].append(metric_func(y_test_all_concat, y_pred_all_concat))\n",
    "\n",
    "for results, test_dataset in zip([results_ppmi, results_adni, results_qpn, results_all], ['ppmi', 'adni', 'qpn', 'all']):\n",
    "# for results, test_dataset in zip([results_ppmi, results_qpn, results_all], ['ppmi', 'qpn', 'all']):\n",
    "    for metric in get_metrics_map():\n",
    "        add_scores('silo', test_dataset, metric, results[f'test_{metric}'], is_null=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(12, 3))\n",
    "\n",
    "# for i_ax, (y_test, y_pred) in enumerate(zip(y_test_all, y_pred_all)):\n",
    "#     ax = axes[i_ax]\n",
    "#     ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\n",
    "#     ax.set_title(['ppmi', 'adni', 'qpn'][i_ax])\n",
    "#     # print('-----')\n",
    "#     # print(f'{score_fn(y_test, y_pred):.3f} ({len(y_test)})')\n",
    "#     # manual_avg += score_fn(y_test, y_pred) * len(y_test)\n",
    "#     # assert y_test_all_concat.loc[y_test.index].equals(y_test)\n",
    "#     # assert y_pred_all_concat.loc[y_pred.index].equals(y_pred)\n",
    "\n",
    "# ConfusionMatrixDisplay.from_predictions(y_test_all_concat, y_pred_all_concat, ax=axes[-1])\n",
    "# axes[-1].set_title('all')\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual_avg = 0\n",
    "# if PROBLEM_TYPE == 'classification':\n",
    "#     score_fn = get_metrics_map()['balanced_accuracy']\n",
    "# elif PROBLEM_TYPE == 'regression':\n",
    "#     score_fn = get_metrics_map()['mean_absolute_error']\n",
    "# for y_test, y_pred in zip(y_test_all, y_pred_all):\n",
    "#     print('-----')\n",
    "#     print(f'{score_fn(y_test, y_pred):.3f} ({len(y_test)})')\n",
    "#     manual_avg += score_fn(y_test, y_pred) * len(y_test)\n",
    "#     assert y_test_all_concat.loc[y_test.index].equals(y_test)\n",
    "#     assert y_pred_all_concat.loc[y_pred.index].equals(y_pred)\n",
    "# print('-----')\n",
    "# print(f'{score_fn(y_test_all_concat, y_pred_all_concat):.3f} ({len(y_test_all_concat)})')\n",
    "# print(manual_avg / len(y_test_all_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF_RESULTS_TO_SAVE.query('method == \"silo\" and test_dataset == \"all\" and metric == \"mean_absolute_error\"')\n",
    "# DF_RESULTS_TO_SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"===== PPMI (NULL) =====\")\n",
    "results_ppmi_null = ml_helper(df_ppmi, pipeline, null=True)\n",
    "\n",
    "print(f\"===== ADNI (NULL) =====\")\n",
    "results_adni_null = ml_helper(df_adni, pipeline, null=True)\n",
    "\n",
    "print(f\"===== QPN (NULL) =====\")\n",
    "results_qpn_null = ml_helper(df_qpn, pipeline, null=True)\n",
    "\n",
    "for results, test_dataset in zip([results_ppmi_null, results_adni_null, results_qpn_null], ['ppmi', 'adni', 'qpn']):\n",
    "# for results, test_dataset in zip([results_ppmi_null, results_qpn_null], ['ppmi', 'qpn']):\n",
    "    for metric in get_metrics_map().keys():\n",
    "        add_scores('silo', test_dataset, metric, results[f'test_{metric}'], is_null=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: aggregated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = ml_helper(df_all, pipeline, null=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "# if PROBLEM_TYPE == 'classification':\n",
    "#     model_null = DummyClassifier()\n",
    "#     pipeline_null = tabular_learner(model_null)\n",
    "#     pipeline_null.set_params(simpleimputer__add_indicator=False, standardscaler='passthrough')\n",
    "# elif PROBLEM_TYPE == 'regression':\n",
    "#     model_null = DummyRegressor(strategy='mean')\n",
    "#     pipeline_null = tabular_learner(model_null)\n",
    "#     if 'simpleimputer' in pipeline.named_steps:\n",
    "#         pipeline_null.set_params(simpleimputer__add_indicator=False)\n",
    "\n",
    "# results_all_null = ml_helper(df_all, pipeline_null)\n",
    "\n",
    "# # for metric in get_metrics_map().keys():\n",
    "# #     add_scores('mega', 'all', metric, results[f'test_{metric}'], is_null=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "\n",
    "def mega_helper(null=False):\n",
    "\n",
    "    metric_map = get_metrics_map()\n",
    "\n",
    "    results_mega = defaultdict(lambda: defaultdict(list))\n",
    "    for i_fold in range(N_FOLDS):\n",
    "        global estimator_mega\n",
    "        estimator_mega = clone(pipeline)\n",
    "        # estimator_ppmi = results_ppmi['estimator'][i_fold]\n",
    "        # estimator_adni = results_adni['estimator'][i_fold]\n",
    "        # estimator_qpn = results_qpn['estimator'][i_fold]\n",
    "        indices_train_ppmi = results_ppmi['indices']['train'][i_fold]\n",
    "        indices_train_adni = results_adni['indices']['train'][i_fold]\n",
    "        indices_train_qpn = results_qpn['indices']['train'][i_fold]\n",
    "        global X_train, y_train\n",
    "        X_train = pd.concat({\n",
    "            'ppmi': results_ppmi['X'].iloc[indices_train_ppmi],\n",
    "            'adni': results_adni['X'].iloc[indices_train_adni],\n",
    "            'qpn': results_qpn['X'].iloc[indices_train_qpn],\n",
    "            # 'ppmi': pd.DataFrame(estimator_ppmi[:-1].transform(results_ppmi['X'].iloc[indices_train_ppmi]), columns=results_ppmi['X'].columns, index=results_ppmi['X'].iloc[indices_train_ppmi].index),\n",
    "            # 'adni': pd.DataFrame(estimator_adni[:-1].transform(results_adni['X'].iloc[indices_train_adni]), columns=results_adni['X'].columns, index=results_adni['X'].iloc[indices_train_adni].index),\n",
    "            # 'qpn': pd.DataFrame(estimator_qpn[:-1].transform(results_qpn['X'].iloc[indices_train_qpn]), columns=results_qpn['X'].columns, index=results_qpn['X'].iloc[indices_train_qpn].index),\n",
    "        })\n",
    "        X_train.index.names = ['dataset', X_train.index.names[-1]]\n",
    "        y_train = pd.concat({\n",
    "            'ppmi': results_ppmi['y'].iloc[indices_train_ppmi],\n",
    "            'adni': results_adni['y'].iloc[indices_train_adni],\n",
    "            'qpn': results_qpn['y'].iloc[indices_train_qpn],\n",
    "        })\n",
    "        y_train.index.names = ['dataset', y_train.index.names[-1]]\n",
    "        if null:\n",
    "            # print('Shuffling y_train')\n",
    "            y_train = y_train.sample(frac=1, replace=False, random_state=RNG_SEED)\n",
    "        # y_train =  y_train.sample(frac=1, replace=False, random_state=RNG_SEED)\n",
    "        # if not null:\n",
    "        #     X_train = X_train.loc[y_train.index]\n",
    "            \n",
    "        estimator_mega.fit(X_train, y_train)\n",
    "\n",
    "        dataset_map = {\n",
    "            'ppmi': results_ppmi,\n",
    "            'adni': results_adni,\n",
    "            'qpn': results_qpn,\n",
    "        }\n",
    "        for label, results in dataset_map.items():\n",
    "            # estimator = results['estimator'][i_fold]\n",
    "            idx_test = results[\"indices\"][\"test\"][i_fold]\n",
    "            global X_test, y_test\n",
    "            # X_test = estimator[:-1].transform(results['X'].iloc[idx_test])\n",
    "            X_test = results['X'].iloc[idx_test]\n",
    "            y_test = results['y'].iloc[idx_test]\n",
    "\n",
    "            for metric_name, metric_func in metric_map.items():\n",
    "                if metric_name == 'roc_auc':\n",
    "                    y_score = estimator_mega.predict_proba(X_test)[:, 1]\n",
    "                    metric = metric_func(y_test, y_score)\n",
    "                else:\n",
    "                    global y_pred\n",
    "                    y_pred = estimator_mega.predict(X_test)\n",
    "                    metric = metric_func(y_test, y_pred)\n",
    "                results_mega[label][metric_name].append(metric)\n",
    "\n",
    "        indices_test_ppmi = results_ppmi['indices']['test'][i_fold]\n",
    "        indices_test_adni = results_adni['indices']['test'][i_fold]\n",
    "        indices_test_qpn = results_qpn['indices']['test'][i_fold]\n",
    "        # global X_test_all, y_test_all\n",
    "        X_test_all = pd.concat({\n",
    "            'ppmi': results_ppmi['X'].iloc[indices_test_ppmi],\n",
    "            'adni': results_adni['X'].iloc[indices_test_adni],\n",
    "            'qpn': results_qpn['X'].iloc[indices_test_qpn],\n",
    "            # 'ppmi': pd.DataFrame(estimator_ppmi[:-1].transform(results_ppmi['X'].iloc[indices_test_ppmi]), columns=results_ppmi['X'].columns, index=results_ppmi['X'].iloc[indices_test_ppmi].index),\n",
    "            # 'adni': pd.DataFrame(estimator_adni[:-1].transform(results_adni['X'].iloc[indices_test_adni]), columns=results_adni['X'].columns, index=results_adni['X'].iloc[indices_test_adni].index),\n",
    "            # 'qpn': pd.DataFrame(estimator_qpn[:-1].transform(results_qpn['X'].iloc[indices_test_qpn]), columns=results_qpn['X'].columns, index=results_qpn['X'].iloc[indices_test_qpn].index),\n",
    " \n",
    "        })\n",
    "        X_test_all.index.names = ['dataset', X_test_all.index.names[-1]]\n",
    "        global y_test_all\n",
    "        y_test_all = pd.concat({\n",
    "            'ppmi': results_ppmi['y'].iloc[indices_test_ppmi],\n",
    "            'adni': results_adni['y'].iloc[indices_test_adni],\n",
    "            'qpn': results_qpn['y'].iloc[indices_test_qpn],\n",
    "        })\n",
    "        y_test_all.index.names = ['dataset', y_test_all.index.names[-1]]\n",
    "        for metric_name, metric_func in metric_map.items():\n",
    "            if metric_name == 'roc_auc':\n",
    "                y_test_all_score = estimator_mega.predict_proba(X_test_all)[:, 1]\n",
    "                metric_test = metric_func(y_test_all, y_test_all_score)\n",
    "                y_train_score = estimator_mega.predict_proba(X_train)[:, 1]\n",
    "                metric_train = metric_func(y_train, y_train_score)\n",
    "            else:\n",
    "                global y_test_all_pred\n",
    "                y_test_all_pred = pd.Series(estimator_mega.predict(X_test_all))\n",
    "                y_test_all_pred.index = y_test_all.index\n",
    "                metric_test = metric_func(y_test_all, y_test_all_pred)\n",
    "                y_train_pred = pd.Series(estimator_mega.predict(X_train))\n",
    "                y_train_pred.index = y_train.index\n",
    "                metric_train = metric_func(y_train, y_train_pred)\n",
    "            results_mega['all'][metric_name].append(metric_test)\n",
    "            results_mega['train'][metric_name].append(metric_train)\n",
    "        # break\n",
    "\n",
    "    for dataset_name in results_mega:\n",
    "        print(f\"===== {dataset_name.upper()} =====\")\n",
    "        for metric_name, metric_values in results_mega[dataset_name].items():\n",
    "            metric_values = np.array(metric_values)\n",
    "            print(f\"{metric_name}: {metric_values.mean():.2f} ({metric_values.std():.2f})\\t{metric_values}\")\n",
    "            if dataset_name not in ['train']:\n",
    "                # add_scores('mega-split_test', dataset_name, metric_name, metric_values, is_null=null)\n",
    "                add_scores('mega', dataset_name, metric_name, metric_values, is_null=null)\n",
    "\n",
    "mega_helper(null=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # manual_avg = 0\n",
    "# # if PROBLEM_TYPE == 'classification':\n",
    "# #     score_fn = get_metrics_map()['balanced_accuracy']\n",
    "# # elif PROBLEM_TYPE == 'regression':\n",
    "# #     score_fn = get_metrics_map()['mean_absolute_error']\n",
    "\n",
    "# # for dataset in ['ppmi', 'adni', 'qpn']:\n",
    "# #     y_test = y_test_all.loc[dataset]\n",
    "# #     y_pred = y_test_all_pred.loc[dataset]\n",
    "# #     print('-----')\n",
    "# #     print(f'{score_fn(y_test, y_pred):.3f} ({len(y_test)})')\n",
    "# #     manual_avg += score_fn(y_test, y_pred) * len(y_test)\n",
    "# #     # assert y_test_all_concat.loc[y_test.index].equals(y_test)\n",
    "# #     # assert y_pred_all_concat.loc[y_pred.index].equals(y_pred)\n",
    "# # print('-----')\n",
    "# # print(f'{score_fn(y_test_all, y_test_all_pred):.3f} ({len(y_test_all)})')\n",
    "# # print(manual_avg / len(y_test_all))\n",
    "\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(12, 3))\n",
    "\n",
    "# for i_ax, dataset in enumerate(['ppmi', 'adni', 'qpn']):\n",
    "# # for i_ax, (y_test, y_pred) in enumerate(zip(y_test_all, y_pred_all)):\n",
    "#     ax = axes[i_ax]\n",
    "#     y_test = y_test_all.loc[dataset]\n",
    "#     y_pred = y_test_all_pred.loc[dataset]\n",
    "#     ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\n",
    "#     ax.set_title(['ppmi', 'adni', 'qpn'][i_ax])\n",
    "#     # print('-----')\n",
    "#     # print(f'{score_fn(y_test, y_pred):.3f} ({len(y_test)})')\n",
    "#     # manual_avg += score_fn(y_test, y_pred) * len(y_test)\n",
    "#     # assert y_test_all_concat.loc[y_test.index].equals(y_test)\n",
    "#     # assert y_pred_all_concat.loc[y_pred.index].equals(y_pred)\n",
    "\n",
    "# ConfusionMatrixDisplay.from_predictions(y_test_all, y_test_all_pred, ax=axes[-1])\n",
    "# axes[-1].set_title('all')\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "# y_test_all_pred.name = 'y_pred'\n",
    "# y_test_all.name = 'y_test'\n",
    "# df_tmp = pd.merge(y_test_all, y_test_all_pred, left_index=True, right_index=True)\n",
    "# # pd.concat(\n",
    "# #     {\n",
    "# #         'y_test': y_test_all.reset_index(),\n",
    "# #         'y_pred': y_test_all_pred.reset_index(),\n",
    "# #     },\n",
    "# # ).reset_index(level=0)\n",
    "# ax = sns.scatterplot(data=df_tmp, x='y_test', y='y_pred', hue='dataset')\n",
    "# ax.set_xlim(55, 87)\n",
    "# ax.set_ylim(55, 87)\n",
    "# ax.plot([55, 87], [55, 87], color='grey', linestyle='--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp = pd.merge(y_test_all, y_test_all_pred, left_index=True, right_index=True)\n",
    "# df_tmp['diff'] = (df_tmp['y_pred'] - df_tmp['y_test']).abs()\n",
    "# df_tmp.reset_index().groupby('dataset')['diff'].mean()\n",
    "# # df_tmp = df_tmp.sort_values('diff', ascending=False)\n",
    "# # print(df_tmp['diff'].mean())\n",
    "# # df_tmp = df_tmp.query('diff > 4.6')\n",
    "# # print(df_tmp.reset_index()['dataset'].value_counts())\n",
    "# # sns.stripplot(data=df_tmp, y='diff', hue='dataset')\n",
    "# # df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5, 5))\n",
    "# ax.scatter(y_test_all, y_test_all_pred, color=y_test_all.index.get_level_values('dataset').map({'ppmi': 'red', 'adni': 'blue', 'qpn': 'green'}))\n",
    "# ax.set_xlim(20, 95)\n",
    "# ax.set_ylim(20, 95)\n",
    "# ax.plot([20, 95], [20, 95], color='black', linestyle='--', zorder=-1)\n",
    "# ax.set_xlabel('true')\n",
    "# ax.set_ylabel('predicted')\n",
    "# fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat({'true': y_test_all['adni'], 'pred': y_test_all_pred['adni']}, axis=1)\n",
    "# df['diff'] = (df['true'] - df['pred']).abs()\n",
    "# df = df.sort_values('diff', ascending=False)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array_equal(y_test_all_pred['adni'].values, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(r2_score(\n",
    "#     y_test,\n",
    "#     results_adni['estimator'][-1].predict(X_test),\n",
    "# ))\n",
    "# print(r2_score(\n",
    "#     y_test,\n",
    "#     estimator_mega.predict(X_test),\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results_ppmi['estimator'][-1][-1].alpha_)\n",
    "# print(results_adni['estimator'][-1][-1].alpha_)\n",
    "# print(results_qpn['estimator'][-1][-1].alpha_)\n",
    "# print(estimator_mega[-1].alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# cols = ['Left-Lateral-Ventricle', 'Right-Lateral-Ventricle', 'Right-Hippocampus', 'Left-Hippocampus']\n",
    "# fig, axes = plt.subplots(1, len(cols), figsize=(15, 5))\n",
    "# for col, ax in zip(cols, axes):\n",
    "#     x = pd.concat([X_train[col], X_test[col]])\n",
    "#     y = pd.concat([y_train, y_test])\n",
    "#     ax.scatter(x, y, alpha=0.5, c=[0]*len(X_train) + [1]*len(X_test))\n",
    "#     ax.set_title(col)\n",
    "# fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred})\n",
    "# df_tmp['diff'] = (df_tmp['y_pred'] - df_tmp['y_test']).abs()\n",
    "# df_tmp.sort_values('diff', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in ['Left-Lateral-Ventricle', 'Right-Lateral-Ventricle', 'Right-Hippocampus', 'Left-Hippocampus']:\n",
    "#     print(f'{col}: {pearsonr(y_train, X_train[col])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in ['Left-Lateral-Ventricle', 'Right-Lateral-Ventricle', 'Right-Hippocampus', 'Left-Hippocampus']:\n",
    "#     print(f'{col}: {pearsonr(y_test, X_test[col])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== ALL DATASETS (NULL) =====\")\n",
    "mega_helper(null=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: federated learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "metric_map = get_metrics_map(include_roc_auc=False)\n",
    "\n",
    "X_fake, y_fake = get_fake_X_y()\n",
    "\n",
    "results_fl_voting = defaultdict(lambda: defaultdict(list))\n",
    "for i_fold in range(N_FOLDS):\n",
    "\n",
    "    estimator_ppmi = results_ppmi[\"estimator\"][i_fold]\n",
    "    estimator_adni = results_adni[\"estimator\"][i_fold]\n",
    "    estimator_qpn = results_qpn[\"estimator\"][i_fold]\n",
    "\n",
    "    estimators = [\n",
    "        (\"ppmi\", estimator_ppmi),\n",
    "        (\"adni\", estimator_adni),\n",
    "        (\"qpn\", estimator_qpn)\n",
    "    ]\n",
    "\n",
    "    if PROBLEM_TYPE == 'classification':\n",
    "        score_ppmi = results_ppmi[\"test_balanced_accuracy\"][i_fold]\n",
    "        score_adni = results_adni[\"test_balanced_accuracy\"][i_fold]\n",
    "        score_qpn = results_qpn[\"test_balanced_accuracy\"][i_fold]\n",
    "        scores_all = np.array([score_ppmi, score_adni, score_qpn])\n",
    "        # weights = (scores_all-0.5) / scores_all.sum()\n",
    "        weights = softmax(scores_all)\n",
    "\n",
    "        # n_samples_ppmi = len(results_ppmi['indices']['test'][i_fold])\n",
    "        # n_samples_adni = len(results_adni['indices']['test'][i_fold])\n",
    "        # n_samples_qpn = len(results_qpn['indices']['test'][i_fold])\n",
    "        # n_samples_all = np.array([n_samples_ppmi, n_samples_adni, n_samples_qpn])\n",
    "        # weights = n_samples_all / n_samples_all.sum()\n",
    "        \n",
    "        voter = VotingClassifier(\n",
    "            estimators=estimators,\n",
    "            voting=\"hard\",\n",
    "            weights=weights,\n",
    "        )\n",
    "    elif PROBLEM_TYPE == 'regression':\n",
    "        score_ppmi = results_ppmi[\"test_r2\"][i_fold]\n",
    "        score_adni = results_adni[\"test_r2\"][i_fold]\n",
    "        score_qpn = results_qpn[\"test_r2\"][i_fold]\n",
    "        scores_all = np.array([score_ppmi, score_adni, score_qpn])\n",
    "        weights = softmax(scores_all)\n",
    "        # weights = scores_all / scores_all.sum()\n",
    "        voter = VotingRegressor(\n",
    "            estimators=estimators,\n",
    "            weights=weights,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"PROBLEM_TYPE must be either 'classification' or 'regression'\")\n",
    "\n",
    "    voter.fit(X_fake, y_fake)  # not sure this is valid\n",
    "    voter.estimators_ = [estimator for _, estimator in estimators]\n",
    "\n",
    "    dataset_map = {\n",
    "        'ppmi': (results_ppmi, df_ppmi),\n",
    "        'adni': (results_adni, df_adni),\n",
    "        'qpn': (results_qpn, df_qpn),\n",
    "    }\n",
    "    for metric_name, metric_func in metric_map.items():\n",
    "        y_test_all = []\n",
    "        y_pred_all = []\n",
    "        for label, (results, df_data) in dataset_map.items():\n",
    "            idx_test = results[\"indices\"][\"test\"][i_fold]\n",
    "            X_test = results['X'].iloc[idx_test]\n",
    "            y_test = results['y'].iloc[idx_test]\n",
    "            y_pred = voter.predict(X_test)\n",
    "            metric = metric_func(y_test, y_pred)\n",
    "            results_fl_voting[label][metric_name].append(metric)\n",
    "            y_test_all.append(y_test)\n",
    "            y_pred_all.append(y_pred)\n",
    "        y_test_all = pd.concat(y_test_all)\n",
    "        y_pred_all = np.concatenate(y_pred_all)\n",
    "        results_fl_voting['all'][metric_name].append(metric_func(y_test_all, y_pred_all))\n",
    "\n",
    "for dataset_name in results_fl_voting:\n",
    "    print(f\"===== {dataset_name} =====\")\n",
    "    for metric_name, metric_values in results_fl_voting[dataset_name].items():\n",
    "        metric_values = np.array(metric_values)\n",
    "        print(f\"{metric_name}: {metric_values.mean():.2f} ({metric_values.std():.2f})\\t{metric_values}\")\n",
    "        add_scores('fl_voting', dataset_name, metric_name, metric_values, is_null=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred1 = estimator_ppmi.predict(X_test)\n",
    "# y_pred2 = estimator_adni.predict(X_test)\n",
    "# y_pred3 = estimator_qpn.predict(X_test)\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     'ppmi': y_pred1,\n",
    "#     'adni': y_pred2,\n",
    "#     'qpn': y_pred3,\n",
    "#     'voter': voter.predict(X_test),\n",
    "#     'true': y_test,\n",
    "# })\n",
    "# print(balanced_accuracy_score(df['true'], df['voter']))\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=N_FOLDS, figsize=(2*N_FOLDS, 15))\n",
    "\n",
    "metric_map = get_metrics_map(include_roc_auc=True)\n",
    "\n",
    "results_fl_avg = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "X_fake, y_fake = get_fake_X_y()\n",
    "\n",
    "dfs_coefs = []\n",
    "\n",
    "for i_fold in range(5):\n",
    "    estimator_avg = clone(pipeline)\n",
    "    estimator_avg.fit(X_fake, y_fake)\n",
    "\n",
    "    n_samples_ppmi = len(results_ppmi['indices']['test'][i_fold])\n",
    "    n_samples_adni = len(results_adni['indices']['test'][i_fold])\n",
    "    n_samples_qpn = len(results_qpn['indices']['test'][i_fold])\n",
    "    n_samples_all = np.array([n_samples_ppmi, n_samples_adni, n_samples_qpn])\n",
    "    # n_samples_all = np.array([n_samples_ppmi, n_samples_qpn])\n",
    "    weights = n_samples_all / n_samples_all.sum()\n",
    "\n",
    "    if 'standardscaler' in estimator_avg.named_steps and estimator_avg['standardscaler'] != 'passthrough':\n",
    "        scaler_ppmi = results_ppmi['estimator'][i_fold]['standardscaler']\n",
    "        scaler_adni = results_adni['estimator'][i_fold]['standardscaler']\n",
    "        scaler_qpn = results_qpn['estimator'][i_fold]['standardscaler']\n",
    "        scaler_avg = estimator_avg['standardscaler']\n",
    "\n",
    "        scales_all = np.vstack([scaler_ppmi.scale_, scaler_adni.scale_, scaler_qpn.scale_])\n",
    "        means_all = np.vstack([scaler_ppmi.mean_, scaler_adni.mean_, scaler_qpn.mean_])\n",
    "        # scales_all = np.vstack([scaler_ppmi.scale_, scaler_qpn.scale_])\n",
    "        # means_all = np.vstack([scaler_ppmi.mean_, scaler_qpn.mean_])\n",
    "\n",
    "        scaler_avg.scale_ = np.average(scales_all, axis=0, weights=weights, keepdims=True)\n",
    "        scaler_avg.mean_ = np.average(means_all, axis=0, weights=weights, keepdims=True)\n",
    "\n",
    "    lr_avg = estimator_avg[-1]\n",
    "    lr_ppmi = results_ppmi[\"estimator\"][i_fold][-1]\n",
    "    lr_adni = results_adni[\"estimator\"][i_fold][-1]\n",
    "    lr_qpn = results_qpn[\"estimator\"][i_fold][-1]\n",
    "\n",
    "    coefs_all = np.vstack([lr_ppmi.coef_, lr_adni.coef_, lr_qpn.coef_])\n",
    "    intercepts_all = np.vstack([lr_ppmi.intercept_, lr_adni.intercept_, lr_qpn.intercept_])\n",
    "    # coefs_all = np.vstack([lr_ppmi.coef_, lr_qpn.coef_])\n",
    "    # intercepts_all = np.vstack([lr_ppmi.intercept_, lr_qpn.intercept_])\n",
    "    \n",
    "    if PROBLEM_TYPE == 'regression':\n",
    "        keepdims = False\n",
    "    else:\n",
    "        keepdims = True\n",
    "\n",
    "    lr_avg.intercept_ = np.average(intercepts_all, axis=0, weights=weights, keepdims=False)\n",
    "    lr_avg.coef_ = np.average(coefs_all, axis=0, weights=weights, keepdims=keepdims)\n",
    "\n",
    "    # for plotting\n",
    "    if PROBLEM_TYPE == 'classification':\n",
    "        df_coefs = pd.concat(\n",
    "            {\n",
    "                'ppmi': pd.Series(lr_ppmi.coef_[0], index=X_fake.columns),\n",
    "                'adni': pd.Series(lr_adni.coef_[0], index=X_fake.columns),\n",
    "                'qpn': pd.Series(lr_qpn.coef_[0], index=X_fake.columns),\n",
    "                'fed': pd.Series(lr_avg.coef_[0], index=X_fake.columns),\n",
    "            },\n",
    "            # axis='columns',\n",
    "        ).reset_index()\n",
    "    elif PROBLEM_TYPE == 'regression':\n",
    "        df_coefs = pd.concat(\n",
    "            {\n",
    "                'ppmi': pd.Series(lr_ppmi.coef_, index=X_fake.columns),\n",
    "                'adni': pd.Series(lr_adni.coef_, index=X_fake.columns),\n",
    "                'qpn': pd.Series(lr_qpn.coef_, index=X_fake.columns),\n",
    "                'fed': pd.Series(lr_avg.coef_, index=X_fake.columns),\n",
    "            },\n",
    "            # axis='columns',\n",
    "        ).reset_index()\n",
    "    df_coefs['i_fold'] = i_fold\n",
    "    dfs_coefs.append(df_coefs)\n",
    "\n",
    "    dataset_map = {\n",
    "        'ppmi': (results_ppmi, df_ppmi),\n",
    "        'adni': (results_adni, df_adni),\n",
    "        'qpn': (results_qpn, df_qpn),\n",
    "    }\n",
    "    for metric_name, metric_func in metric_map.items():\n",
    "        y_test_all = []\n",
    "        y_pred_all = []\n",
    "        for label, (results, df_data) in dataset_map.items():\n",
    "            idx_test = results[\"indices\"][\"test\"][i_fold]\n",
    "            X_test = results['X'].iloc[idx_test]\n",
    "            y_test = results['y'].iloc[idx_test]\n",
    "\n",
    "            if metric_name == 'roc_auc':\n",
    "                # technically scores but renaming to pred for concatenation\n",
    "                y_pred = estimator_avg.predict_proba(X_test)[:, 1]\n",
    "                metric = metric_func(y_test, y_pred)\n",
    "            else:\n",
    "                y_pred = estimator_avg.predict(X_test)\n",
    "                metric = metric_func(y_test, y_pred)\n",
    "            results_fl_avg[label][metric_name].append(metric)\n",
    "            y_test_all.append(y_test)\n",
    "            y_pred_all.append(y_pred)\n",
    "        y_test_all = pd.concat(y_test_all)\n",
    "        y_pred_all = np.concatenate(y_pred_all)\n",
    "        results_fl_avg['all'][metric_name].append(metric_func(y_test_all, y_pred_all))\n",
    "\n",
    "for dataset_name in results_fl_avg:\n",
    "    print(f\"===== {dataset_name} =====\")\n",
    "    for metric_name, metric_values in results_fl_avg[dataset_name].items():\n",
    "        metric_values = np.array(metric_values)\n",
    "        print(f\"{metric_name}: {metric_values.mean():.2f} ({metric_values.std():.2f})\\t{metric_values}\")\n",
    "        add_scores('fl_fedavg', dataset_name, metric_name, metric_values, is_null=False)\n",
    "\n",
    "# df_coefs = pd.concat(dfs_coefs)\n",
    "# grid = sns.catplot(data=df_coefs, kind='bar', x='level_1', hue='level_0', y=0, row='i_fold', aspect=3)\n",
    "# grid.tick_params(axis='x', rotation=30)\n",
    "# # grid.set_xticklabels(rotation=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(PROBLEM_TYPE)\n",
    "# print(lr_ppmi.coef_.shape)\n",
    "# print(lr_adni.coef_.shape)\n",
    "# print(lr_qpn.coef_.shape)\n",
    "# print(lr_avg.coef_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_RESULTS_TO_SAVE = DF_RESULTS_TO_SAVE.drop_duplicates()\n",
    "DPATH_RESULTS.mkdir(exist_ok=True)\n",
    "fpath_out = DPATH_RESULTS / f'results-{PROBLEM_TYPE}-{COMMON_TAGS}-{N_FOLDS}-{RNG_SEED}.tsv'\n",
    "DF_RESULTS_TO_SAVE.to_csv(fpath_out, sep='\\t', index=False)\n",
    "# print(fpath_out)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_RESULTS_TO_SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# # df_tmp = DF_RESULTS_TO_SAVE.query('is_null == False and metric == \"mean_absolute_error\" and method != \"fl_voting\" and method != \"fl_fedavg\"')#.groupby(['method', 'test_dataset']).score.mean()\n",
    "# df_tmp = DF_RESULTS_TO_SAVE.query('is_null == False and metric == \"balanced_accuracy\" and method != \"fl_voting\" and method != \"fl_fedavg\"')#.groupby(['method', 'test_dataset']).score.mean()\n",
    "# fg = sns.catplot(\n",
    "#     data = df_tmp,\n",
    "#     hue='test_dataset',\n",
    "#     x='i_fold',\n",
    "#     y='score',\n",
    "#     col='method',\n",
    "#     kind='strip',\n",
    "#     aspect=3,\n",
    "#     height=2,\n",
    "#     sharex=False,\n",
    "# )\n",
    "\n",
    "# for method, ax in fg.axes_dict.items():\n",
    "#     mean = df_tmp.query(f'method == \"{method}\" and test_dataset == \"all\"')['score'].mean()\n",
    "#     std = df_tmp.query(f'method == \"{method}\" and test_dataset == \"all\"')['score'].std()\n",
    "#     ax.set_title(f'{ax.get_title()} (mean: {mean:.2f}, std: {std:.2f})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-pd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
